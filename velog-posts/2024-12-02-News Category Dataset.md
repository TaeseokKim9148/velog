---
title: "News Category Dataset"
date: Mon, 02 Dec 2024 04:49:27 GMT
categories: Velog
link: https://velog.io/@kim_taixi/News-Category-Dataset
---

<h2 id="ë¶€ì œ-news-classification-using-bert">ë¶€ì œ: News Classification using BERT</h2>
<blockquote>
<h2 id="íŒŒì¼-ë°-í•„ë“œ-ì„¤ëª…">íŒŒì¼ ë° í•„ë“œ ì„¤ëª…</h2>
</blockquote>
<p>2012ë…„ë¶€í„° 2022ë…„ê¹Œì§€ì˜ ì•½ 210,000ê°œì˜ ë‰´ìŠ¤ í—¤ë“œë¼ì¸</p>
<p>ë°ì´í„° ì„¸íŠ¸ì˜ ê° ë ˆì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì†ì„±ìœ¼ë¡œ êµ¬ì„±</p>
<ul>
<li>ì¹´í…Œê³ ë¦¬: ê¸°ì‚¬ê°€ ê²Œì¬ëœ ì¹´í…Œê³ ë¦¬</li>
<li>í—¤ë“œë¼ì¸: ë‰´ìŠ¤ ê¸°ì‚¬ì˜ í—¤ë“œë¼ì¸</li>
<li>ì €ì: ê¸°ì‚¬ì— ê¸°ì—¬í•œ ì €ì ëª©ë¡</li>
<li>ë§í¬: ì›ë³¸ ë‰´ìŠ¤ ê¸°ì‚¬ë¡œì˜ ë§í¬</li>
<li>short_description: ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ìš”ì•½</li>
<li>ë‚ ì§œ: ê¸°ì‚¬ê°€ ì¶œíŒëœ ë‚ ì§œ</li>
</ul>
<p>ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ì´ 42ê°œì˜ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬</p>
<p>í˜•ì‹:
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/a41c2e2e-e43e-409a-be00-fce4ae814a3d/image.png" /></p>
<ul>
<li><p>ì˜ì–´ ë¶ˆìš©ì–´ ì‚¬ì „</p>
<ul>
<li>ì˜ˆì‹œ : a, about ,above, after, again, against, all</li>
</ul>
<blockquote>
<h2 id="ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-íŒ¨í‚¤ì§€-ì„¤ì¹˜">ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜</h2>
</blockquote>
<pre><code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('darkgrid')
import nltk
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
import tensorflow as tf
from tensorflow.keras.models import Model, save_model, load_model
from tensorflow.keras.layers import Dropout, Input, Dense, Lambda
from transformers import DistilBertTokenizer, TFAutoModel, AdamW, TFDistilBertModel
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Layer
from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import regex as re
import pickle
import warnings
warnings.filterwarnings('ignore')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')
</code></pre>
</li>
</ul>
<blockquote>
<h2 id="ë°ì´í„°ë¡œë“œ-ë°-ë°ì´í„°-íŒŒì•…">ë°ì´í„°ë¡œë“œ ë° ë°ì´í„° íŒŒì•…</h2>
</blockquote>
<pre><code class="language-python"># ë°ì´í„°ë¡œë”©
news = pd.read_json('/Users/taixi/Desktop/small_project/News Category Dataset/News_Category_Dataset_v3.json', lines=True)
news.head()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/c571a390-af7f-4ed1-a22f-a2e1bcb01224/image.png" /></p>
<blockquote>
<h2 id="eda">EDA</h2>
</blockquote>
<pre><code class="language-python">news = news[['headline', 'short_description', 'category']] # feature selection
news.head()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/c65c8fa1-92c0-4ebb-abb3-3d0be5c5f1e3/image.png" /></p>
<h3 id="category">Category</h3>
<pre><code class="language-python">news.category.unique()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/8593080d-a2a4-404d-8d31-66e98b47ffda/image.png" /></p>
<pre><code class="language-python">news.category = news.category.replace({&quot;HEALTHY LIVING&quot;: &quot;WELLNESS&quot;,
              &quot;QUEER VOICES&quot;: &quot;GROUPS VOICES&quot;,
              &quot;BUSINESS&quot;: &quot;BUSINESS &amp; FINANCES&quot;,
              &quot;PARENTS&quot;: &quot;PARENTING&quot;,
              &quot;BLACK VOICES&quot;: &quot;GROUPS VOICES&quot;,
              &quot;THE WORLDPOST&quot;: &quot;WORLD NEWS&quot;,
              &quot;STYLE&quot;: &quot;STYLE &amp; BEAUTY&quot;,
              &quot;GREEN&quot;: &quot;ENVIRONMENT&quot;,
              &quot;TASTE&quot;: &quot;FOOD &amp; DRINK&quot;,
              &quot;WORLDPOST&quot;: &quot;WORLD NEWS&quot;,
              &quot;SCIENCE&quot;: &quot;SCIENCE &amp; TECH&quot;,
              &quot;TECH&quot;: &quot;SCIENCE &amp; TECH&quot;,
              &quot;MONEY&quot;: &quot;BUSINESS &amp; FINANCES&quot;,
              &quot;ARTS&quot;: &quot;ARTS &amp; CULTURE&quot;,
              &quot;COLLEGE&quot;: &quot;EDUCATION&quot;,
              &quot;LATINO VOICES&quot;: &quot;GROUPS VOICES&quot;,
              &quot;CULTURE &amp; ARTS&quot;: &quot;ARTS &amp; CULTURE&quot;,
              &quot;FIFTY&quot;: &quot;MISCELLANEOUS&quot;,
              &quot;GOOD NEWS&quot;: &quot;MISCELLANEOUS&quot;}
            )

len(news['category'].unique())</code></pre>
<pre><code class="language-python">plt.figure(figsize=(10, 10))
plt.pie(x=news.category.value_counts(), labels=news.category.value_counts().index, autopct='%1.1f%%', textprops={'fontsize' : 8,
                                                                                                                'alpha' : .7});
plt.title('The precentage of instance belonging to each class', alpha=.7);
plt.tight_layout();</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/8e5c0ed4-8e90-4cc4-ac4c-4d39baeddfae/image.png" /></p>
<blockquote>
<h2 id="ì „ì²˜ë¦¬">ì „ì²˜ë¦¬</h2>
</blockquote>
<pre><code class="language-python"># ì˜ì–´ë¶ˆìš©ì–´ì‚¬ì „ ì‚¬ìš©
with open('/Users/taixi/Desktop/small_project/News Category Dataset/EN-Stopwords.txt', 'r') as f:
    stopwords = f.readlines()
    f.close()
stopwords = [re.sub('\n', '', w) for w in stopwords]

def text_preprocessing(df:pd.DataFrame):

    lem = WordNetLemmatizer()
    new_df = pd.DataFrame(columns=['head_desc', 'category'])
    max_len = 0
    for index, row in df.iterrows():
        head_desc = row.headline + &quot; &quot; + row.short_description
        head_desc_tokenized = word_tokenize(head_desc) # Word Tokenization
        punctuation_stopwords_removed = [re.sub(r'[^\w\s]', '', token) for token in head_desc_tokenized if not token in stopwords] # punctuations and stopwords removal
        number_removed = [re.sub(r'\d+', '', token) for token in punctuation_stopwords_removed] # numbers removal
        head_desc_lemmatized = [lem.lemmatize(token) for token in number_removed] # Word Lemmatization
        empty_str_removed = [token for token in head_desc_lemmatized if token != ''] # empty strings removal
        if len(empty_str_removed) &gt; max_len:
            max_len = len(empty_str_removed)
        new_df.loc[index] = {
            'head_desc' : &quot; &quot;.join(empty_str_removed),
            'category' : row['category']
        }
    X, y = new_df['head_desc'], new_df['category']
    return X, y, max_len

X, y, max_len = text_preprocessing(news)

def save_data(name, data):
    with open(name, 'wb') as f:
        pickle.dump(data, f)
        f.close()

save_data('X.h5', X)
save_data('y.h5', y)
save_data('max_len', max_len)

def load_data(name):
    return pickle.load(open(name, 'rb'))

max_len

X.shape, y.shape

y.head()

y = pd.get_dummies(y) # OHE
classes_name = y.columns.tolist()
y.head()

y = y.replace([True, False], [1, 0]).values
y.shape</code></pre>
<p>133
((209527,), (209527,))
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/fd37db03-aec2-4d24-9094-53ca45c70ab9/image.png" /></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/beb4666d-77f8-41e3-bd1c-09a3a86fd25a/image.png" /></p>
<p>(209527, 27)</p>
<h3 id="ë°ì´í„°ë¶„í• ">ë°ì´í„°ë¶„í• </h3>
<pre><code class="language-python">X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=.3, random_state=42)
X_eval, X_test, y_eval, y_test = train_test_split(X_temp, y_temp, test_size=.5, random_state=42)

# create a DistilBertTokenizer object
tokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path=&quot;distilbert-base-uncased&quot;)

save_data('tokenizer.h5', tokenizer)</code></pre>
<h3 id="í† í°í™”">í† í°í™”</h3>
<pre><code class="language-python">def tokenizer_preprocessing(texts, tokenizer):

    encoded_dict = tokenizer.batch_encode_plus(
        texts,
        return_token_type_ids=False,
        pad_to_max_length=True, 
        max_length=max_len
    )
    return np.array(encoded_dict['input_ids'])

padded_train = tokenizer_preprocessing(X_train, tokenizer)
padded_eval = tokenizer_preprocessing(X_eval, tokenizer)
padded_test = tokenizer_preprocessing(X_test, tokenizer)

save_data('padded_train.h5', padded_train)
save_data('padded_eval.h5', padded_eval)
save_data('padded_test.h5', padded_test)

padded_train.shape, padded_eval.shape, padded_test.shape</code></pre>
<p>((146668, 133), (31429, 133), (31430, 133))</p>
<blockquote>
<h2 id="ëª¨ë¸-ì„¤ì •-ë°-í›ˆë ¨">ëª¨ë¸ ì„¤ì • ë° í›ˆë ¨</h2>
</blockquote>
<pre><code class="language-python">import shutil
import os

cache_dir = os.path.expanduser('~/.cache/huggingface/transformers')
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)

import torch_optimizer as optim
from tensorflow.keras.optimizers import Adam

with dist_strategy.scope():
    pretrained_model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')
    if pretrained_model is not None:
        # ëª¨ë¸ ì‚¬ìš©
        print(&quot;ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.&quot;)
    else:
        print(&quot;ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤&quot;)

    # ì…ë ¥ ë ˆì´ì–´ ì •ì˜
    input_ids = Input(shape=(max_len,), dtype='int32', name='input_ids')

    # Lambda ë ˆì´ì–´ì— DistilBERT ëª¨ë¸ ì ìš©
    def apply_distilbert(x):
        return pretrained_model(x)[0]

    # ì¶œë ¥ í˜•íƒœ ê³„ì‚° í•¨ìˆ˜
    def distilbert_output_shape(input_shape):
        return (input_shape[0], max_len, 768)

    # Lambda ë ˆì´ì–´ ì •ì˜
    bert_output = Lambda(apply_distilbert, output_shape=distilbert_output_shape)(input_ids)

    # CLS í† í° ì¶”ì¶œ í•¨ìˆ˜
    def extract_cls_token(x):
        return x[:, 0, :]

    # CLS í† í° ì¶œë ¥ í˜•íƒœ ê³„ì‚° í•¨ìˆ˜
    def cls_output_shape(input_shape):
        return (input_shape[0], 768)

    # CLS í† í° ì¶”ì¶œì„ ìœ„í•œ Lambda ë ˆì´ì–´
    cls_token = Lambda(extract_cls_token, output_shape=cls_output_shape)(bert_output)

    # Dense ë ˆì´ì–´ ì¶”ê°€
    x = Dense(64, activation='relu')(cls_token)

    # ì¶œë ¥ ë ˆì´ì–´ (42ê°œì˜ ë ˆì´ë¸”ì— ëŒ€í•œ ë‹¤ì¤‘ ë ˆì´ë¸” ì´ì§„ ë¶„ë¥˜)
    output_layer = Dense(42, activation='sigmoid')(x)

    # ëª¨ë¸ ì •ì˜
    bert_tf = Model(inputs=input_ids, outputs=output_layer)

    # ëª¨ë¸ ì»´íŒŒì¼ (ë‹¤ì¤‘ ë ˆì´ë¸” ì´ì§„ ë¶„ë¥˜ì— ì í•©í•œ ì†ì‹¤ í•¨ìˆ˜ ì‚¬ìš©)
    bert_tf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # ëª¨ë¸ ìš”ì•½
    bert_tf.summary()

    # ëª¨ë¸ í›ˆë ¨
    EPOCHS = 50
    BATCH_SIZE = 32 * dist_strategy.num_replicas_in_sync  # ì›ë˜ ë°°ì¹˜ í¬ê¸° ì„¤ì •
    STEPS_PER_EPOCH = X_train.shape[0] // BATCH_SIZE  # ì¹˜ ìˆ˜ ê³„ì‚°
    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
    model_checkpoint = ModelCheckpoint(
        '/content//model_weights.keras',  # í™•ì¥ìë¥¼ .kerasë¡œ ë³€ê²½
        monitor='val_f1_score',  # ëª¨ë‹ˆí„°ë§í•  ë©”íŠ¸ë¦­
        save_best_only=True
    )
    lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)

    history = bert_tf.fit(
        padded_train,
        y_train,
        validation_data=(padded_eval, y_eval),
        epochs=EPOCHS,
        steps_per_epoch=572,
        callbacks=[lr, early_stopping, model_checkpoint]
    )</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/ddd84acb-f11a-4e21-8409-0f66540824e5/image.png" /></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/cefd6a8d-b2c1-4fcd-b8b0-38201bd4186d/image.png" /></p>
<pre><code class="language-python">plt.figure(figsize=(7, 7))
plt.plot(history.history['loss'], label='loss');
plt.plot(history.history['val_loss'], label='val_loss');
plt.legend();
plt.title('Loss vs Validation Loss');
plt.tight_layout();

plt.figure(figsize=(7, 7))
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.legend()
plt.title('Accuracy vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.tight_layout()
plt.show()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/58767935-1725-407e-a88e-41cc9351b911/image.png" /></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/7fdffe77-13b0-4d6f-a204-aaeccf173326/image.png" /></p>
<ul>
<li>lossëŠ” ë‚®ì€ë° accuracyëŠ” ìƒê°ë³´ë‹¤ ë†’ì§€ì•Šê¸°ë•Œë¬¸ì— ê°œì„ ì´ í•„ìš”í•¨</li>
</ul>
<pre><code class="language-python">y_pred = bert_tf.predict(padded_test)
y_pred_class = np.argmax(y_pred, axis=1) # ì˜ˆì¸¡ í´ë˜ìŠ¤ ì¶”ì¶œ
y_pred_class

y_test_class = np.argmax(y_test, axis=1)
y_test_class

print(classification_report(y_true=y_test_class, y_pred=y_pred_class))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/80f806f1-f3d4-4e05-be56-c98c4817c306/image.png" /></p>
<pre><code class="language-python">plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges', cbar=False);
plt.tight_layout();
plt.xticks(range(conf_matrix.shape[0]), classes_name, rotation=90);  
plt.yticks(range(conf_matrix.shape[0]), classes_name, rotation=360);</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/09d3165d-4397-41ce-80df-117401d5cfc2/image.png" /></p>
<h3 id="ê²°ë¡ ">ê²°ë¡ </h3>
<p>ğŸ’¡ ìºê¸€ì„ í†µí•´ ë„¤ì´ë²„ ë‰´ìŠ¤ì„ ë¯¸ë¦¬ ê²½í—˜í•¨</p>
