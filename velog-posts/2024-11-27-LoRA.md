---
title: "LoRA"
date: Wed, 27 Nov 2024 08:31:29 GMT
categories: Velog
link: https://velog.io/@kim_taixi/LoRA
---

<blockquote>
<h1 id="캐글에서-lora-학습해보기">캐글에서 LoRA 학습해보기</h1>
</blockquote>
<blockquote>
<h2 id="파일-설명">파일 설명</h2>
</blockquote>
<ul>
<li>브레인스토밍, 분류, 폐쇄형 QA, 생성, 정보 추출, 개방형 QA 및 요약을 포함한 여러 행동 범주에서 수천 명의 Databricks 직원이 생성한 지시 따르기 기록의 오픈 소스 데이터 세트</li>
<li><strong>databricks dolly 15k</strong> </li>
</ul>
<blockquote>
<h2 id="라이브러리-및-패키지-설치">라이브러리 및 패키지 설치</h2>
</blockquote>
<pre><code class="language-python">!pip install -q -U keras-nlp
!pip install -q -U keras&gt;=3

import os
# 딥러닝 프레임워크의 백엔드와 메모리 관리를 설정
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;jax&quot;  # Or &quot;torch&quot; or &quot;tensorflow&quot;.
# Avoid memory fragmentation on JAX backend.
os.environ[&quot;XLA_PYTHON_CLIENT_MEM_FRACTION&quot;]=&quot;1.00&quot;</code></pre>
<ul>
<li>KERAS_BACKEND는 Keras가 어떤 딥러닝 엔진을 사용할지 정하는 설정 </li>
<li>&quot;1.00&quot;은 사용 가능한 메모리의 100%를 사용한다는 의미</li>
</ul>
<p>🖍️ 첫 번째 설정은 &quot;어떤 자동차를 운전할지&quot; 고르는 것과 같음
🖍️두 번째 설정은 &quot;자동차의 연료 탱크를 얼마나 채울지&quot; 정하는 것과 같음</p>
<h4 id="대화형-ai-모델을-훈련시키기-위한-데이터를-준비하는-과정">대화형 AI 모델을 훈련시키기 위한 데이터를 준비하는 과정</h4>
<pre><code class="language-python">import keras
import keras_nlp

import json
data = []
with open('/content/databricks-dolly-15k.jsonl') as file:
    for line in file:
        features = json.loads(line)
        # Filter out examples with context, to keep it simple.
        if features[&quot;context&quot;]:
            continue
        # Format the entire example as a single string.
        template = &quot;Instruction:\n{instruction}\n\nResponse:\n{response}&quot;
        data.append(template.format(**features))
#1000개의 데이터만 뽑아서 사용 
# Only use 1000 training examples, to keep it fast.
data = data[:1000]</code></pre>
<blockquote>
<h2 id="케글api">케글API</h2>
</blockquote>
<pre><code class="language-python">!mkdir -p ~/.kaggle
!echo '{&quot;username&quot;:&quot;taixi1992&quot;,&quot;key&quot;:&quot;ad773ff8285304af746dc0cc8ca5b153&quot;}' &gt; ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.jso</code></pre>
<blockquote>
<h2 id="모델-다운-및-로딩">모델 다운 및 로딩</h2>
</blockquote>
<pre><code class="language-python">import kagglehub
model = kagglehub.model_download(&quot;keras/gemma/keras/gemma_2b_en/2&quot;)

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(&quot;gemma_2b_en&quot;)
gemma_lm.summary()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/4497e651-4c7c-4137-b0c3-0d709d80ee21/image.png" /></p>
<ul>
<li>25억개의 파라미터 갯수 !!! 엄청난 수 </li>
</ul>
<blockquote>
<h2 id="fine-tuning-전">fine tuning 전</h2>
</blockquote>
<pre><code class="language-python">prompt = template.format(
    instruction=&quot;What should I do on a trip to Europe?&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/02932630-a370-4440-9215-18b4f5d27b81/image.png" /></p>
<pre><code class="language-python">prompt = template.format(
    instruction=&quot;Explain the process of photosynthesis in a way that a child could understand.&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/04cf1ef9-05da-474a-8c32-398cb90753b6/image.png" /></p>
<ul>
<li>fine tuning 전이라서 학습이 제대로 안되는게 보임</li>
</ul>
<blockquote>
<h2 id="lora-fine-tuning">LoRA Fine-tuning</h2>
</blockquote>
<pre><code class="language-python">gemma_lm.backbone.enable_lora(rank=8)
gemma_lm.summary()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/a56a1c7b-4456-4942-bde7-96c1cef834b2/image.png" /></p>
<ul>
<li>활성화 파라미터를 엄청나게 줄였음 !!!</li>
</ul>
<pre><code class="language-python"># Limit the input sequence length to 512 (to control memory usage).
gemma_lm.preprocessor.sequence_length = 512
# Use AdamW (a common optimizer for transformer models).
optimizer = keras.optimizers.AdamW(
    learning_rate=5e-5,
    weight_decay=0.01,
)
# Exclude layernorm and bias terms from decay.
optimizer.exclude_from_weight_decay(var_names=[&quot;bias&quot;, &quot;scale&quot;])

gemma_lm.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=optimizer,
    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
)
gemma_lm.fit(data, epochs=1, batch_size=1)</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/b5b7fdcf-d1e1-4f21-beb4-0276a82c16a7/image.png" /></p>
<blockquote>
<h2 id="lora-fine-tuning-결과">LoRA Fine-tuning 결과</h2>
</blockquote>
<pre><code class="language-python"># Rank(4)
prompt = template.format(
    instruction=&quot;What should I do on a trip to Europe?&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/ffe2106c-094b-49b6-a76b-6ee2b98e01e7/image.png" /></p>
<ul>
<li>전보다는 결과가 좋지만 아직부족한듯하다<del>~</del></li>
</ul>
<pre><code class="language-python"># Rank(8)
prompt = template.format(
    instruction=&quot;What should I do on a trip to Europe?&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<ul>
<li>랭크를 변경해서 조금더 섬세하게 미세조정을 해봄 ~
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/1ba47980-0fea-44b9-a0d6-5ef86d71e623/image.png" /></li>
</ul>
<pre><code class="language-python"># Rank(4)
prompt = template.format(
    instruction=&quot;Explain the process of photosynthesis in a way that a child could understand.&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/aff02c98-7342-41bc-83ea-9d0229bef418/image.png" /></p>
<pre><code class="language-python">
# Rank(8)
prompt = template.format(
    instruction=&quot;Explain the process of photosynthesis in a way that a child could understand.&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/7950ae42-5584-4570-a19f-84c90a9d3c3a/image.png" /></p>
<p>랭크4보다는 랭크8로 변경시 결과가 더 좋은거 같다 <del>~</del></p>
<p><strong>💡 높은 LoRA 순위</strong>는 모델의 세부 조정과 성능 향상에 효과적이며, 복잡한 작업과 데이터셋에서 특히 유용함. 하지만 리소스와 성능 간의 균형을 고려해야 함.</p>
<ul>
<li>LoRA를 활성화하면 학습 가능한 매개변수의 수가 크게 감소함</li>
<li>학습 가능한 매개변수의 수가 <strong>25억 개에서 130만 개</strong>로 줄어들음</li>
</ul>
