---
title: "LoRA"
date: Wed, 27 Nov 2024 08:31:29 GMT
categories: Velog
link: https://velog.io/@kim_taixi/LoRA
---

<blockquote>
<h1 id="ìºê¸€ì—ì„œ-lora-í•™ìŠµí•´ë³´ê¸°">ìºê¸€ì—ì„œ LoRA í•™ìŠµí•´ë³´ê¸°</h1>
</blockquote>
<blockquote>
<h2 id="íŒŒì¼-ì„¤ëª…">íŒŒì¼ ì„¤ëª…</h2>
</blockquote>
<ul>
<li>ë¸Œë ˆì¸ìŠ¤í† ë°, ë¶„ë¥˜, íì‡„í˜• QA, ìƒì„±, ì •ë³´ ì¶”ì¶œ, ê°œë°©í˜• QA ë° ìš”ì•½ì„ í¬í•¨í•œÂ ì—¬ëŸ¬ í–‰ë™ ë²”ì£¼ì—ì„œ ìˆ˜ì²œ ëª…ì˜ Databricks ì§ì›ì´ ìƒì„±í•œ ì§€ì‹œ ë”°ë¥´ê¸° ê¸°ë¡ì˜ ì˜¤í”ˆ ì†ŒìŠ¤ ë°ì´í„° ì„¸íŠ¸</li>
<li><strong>databricks dolly 15k</strong> </li>
</ul>
<blockquote>
<h2 id="ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-íŒ¨í‚¤ì§€-ì„¤ì¹˜">ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜</h2>
</blockquote>
<pre><code class="language-python">!pip install -q -U keras-nlp
!pip install -q -U keras&gt;=3

import os
# ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì˜ ë°±ì—”ë“œì™€ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ì„¤ì •
os.environ[&quot;KERAS_BACKEND&quot;] = &quot;jax&quot;  # Or &quot;torch&quot; or &quot;tensorflow&quot;.
# Avoid memory fragmentation on JAX backend.
os.environ[&quot;XLA_PYTHON_CLIENT_MEM_FRACTION&quot;]=&quot;1.00&quot;</code></pre>
<ul>
<li>KERAS_BACKENDëŠ” Kerasê°€ ì–´ë–¤ ë”¥ëŸ¬ë‹ ì—”ì§„ì„ ì‚¬ìš©í• ì§€ ì •í•˜ëŠ” ì„¤ì • </li>
<li>&quot;1.00&quot;ì€ ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ì˜ 100%ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì˜ë¯¸</li>
</ul>
<p>ğŸ–ï¸ ì²« ë²ˆì§¸ ì„¤ì •ì€ &quot;ì–´ë–¤ ìë™ì°¨ë¥¼ ìš´ì „í• ì§€&quot; ê³ ë¥´ëŠ” ê²ƒê³¼ ê°™ìŒ
ğŸ–ï¸ë‘ ë²ˆì§¸ ì„¤ì •ì€ &quot;ìë™ì°¨ì˜ ì—°ë£Œ íƒ±í¬ë¥¼ ì–¼ë§ˆë‚˜ ì±„ìš¸ì§€&quot; ì •í•˜ëŠ” ê²ƒê³¼ ê°™ìŒ</p>
<h4 id="ëŒ€í™”í˜•-ai-ëª¨ë¸ì„-í›ˆë ¨ì‹œí‚¤ê¸°-ìœ„í•œ-ë°ì´í„°ë¥¼-ì¤€ë¹„í•˜ëŠ”-ê³¼ì •">ëŒ€í™”í˜• AI ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ê³¼ì •</h4>
<pre><code class="language-python">import keras
import keras_nlp

import json
data = []
with open('/content/databricks-dolly-15k.jsonl') as file:
    for line in file:
        features = json.loads(line)
        # Filter out examples with context, to keep it simple.
        if features[&quot;context&quot;]:
            continue
        # Format the entire example as a single string.
        template = &quot;Instruction:\n{instruction}\n\nResponse:\n{response}&quot;
        data.append(template.format(**features))
#1000ê°œì˜ ë°ì´í„°ë§Œ ë½‘ì•„ì„œ ì‚¬ìš© 
# Only use 1000 training examples, to keep it fast.
data = data[:1000]</code></pre>
<blockquote>
<h2 id="ì¼€ê¸€api">ì¼€ê¸€API</h2>
</blockquote>
<pre><code class="language-python">!mkdir -p ~/.kaggle
!echo '{&quot;username&quot;:&quot;taixi1992&quot;,&quot;key&quot;:&quot;ad773ff8285304af746dc0cc8ca5b153&quot;}' &gt; ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.jso</code></pre>
<blockquote>
<h2 id="ëª¨ë¸-ë‹¤ìš´-ë°-ë¡œë”©">ëª¨ë¸ ë‹¤ìš´ ë° ë¡œë”©</h2>
</blockquote>
<pre><code class="language-python">import kagglehub
model = kagglehub.model_download(&quot;keras/gemma/keras/gemma_2b_en/2&quot;)

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(&quot;gemma_2b_en&quot;)
gemma_lm.summary()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/4497e651-4c7c-4137-b0c3-0d709d80ee21/image.png" /></p>
<ul>
<li>25ì–µê°œì˜ íŒŒë¼ë¯¸í„° ê°¯ìˆ˜ !!! ì—„ì²­ë‚œ ìˆ˜ </li>
</ul>
<blockquote>
<h2 id="fine-tuning-ì „">fine tuning ì „</h2>
</blockquote>
<pre><code class="language-python">prompt = template.format(
    instruction=&quot;What should I do on a trip to Europe?&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/02932630-a370-4440-9215-18b4f5d27b81/image.png" /></p>
<pre><code class="language-python">prompt = template.format(
    instruction=&quot;Explain the process of photosynthesis in a way that a child could understand.&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/04cf1ef9-05da-474a-8c32-398cb90753b6/image.png" /></p>
<ul>
<li>fine tuning ì „ì´ë¼ì„œ í•™ìŠµì´ ì œëŒ€ë¡œ ì•ˆë˜ëŠ”ê²Œ ë³´ì„</li>
</ul>
<blockquote>
<h2 id="lora-fine-tuning">LoRA Fine-tuning</h2>
</blockquote>
<pre><code class="language-python">gemma_lm.backbone.enable_lora(rank=8)
gemma_lm.summary()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/a56a1c7b-4456-4942-bde7-96c1cef834b2/image.png" /></p>
<ul>
<li>í™œì„±í™” íŒŒë¼ë¯¸í„°ë¥¼ ì—„ì²­ë‚˜ê²Œ ì¤„ì˜€ìŒ !!!</li>
</ul>
<pre><code class="language-python"># Limit the input sequence length to 512 (to control memory usage).
gemma_lm.preprocessor.sequence_length = 512
# Use AdamW (a common optimizer for transformer models).
optimizer = keras.optimizers.AdamW(
    learning_rate=5e-5,
    weight_decay=0.01,
)
# Exclude layernorm and bias terms from decay.
optimizer.exclude_from_weight_decay(var_names=[&quot;bias&quot;, &quot;scale&quot;])

gemma_lm.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=optimizer,
    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
)
gemma_lm.fit(data, epochs=1, batch_size=1)</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/b5b7fdcf-d1e1-4f21-beb4-0276a82c16a7/image.png" /></p>
<blockquote>
<h2 id="lora-fine-tuning-ê²°ê³¼">LoRA Fine-tuning ê²°ê³¼</h2>
</blockquote>
<pre><code class="language-python"># Rank(4)
prompt = template.format(
    instruction=&quot;What should I do on a trip to Europe?&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/ffe2106c-094b-49b6-a76b-6ee2b98e01e7/image.png" /></p>
<ul>
<li>ì „ë³´ë‹¤ëŠ” ê²°ê³¼ê°€ ì¢‹ì§€ë§Œ ì•„ì§ë¶€ì¡±í•œë“¯í•˜ë‹¤<del>~</del></li>
</ul>
<pre><code class="language-python"># Rank(8)
prompt = template.format(
    instruction=&quot;What should I do on a trip to Europe?&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<ul>
<li>ë­í¬ë¥¼ ë³€ê²½í•´ì„œ ì¡°ê¸ˆë” ì„¬ì„¸í•˜ê²Œ ë¯¸ì„¸ì¡°ì •ì„ í•´ë´„ ~
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/1ba47980-0fea-44b9-a0d6-5ef86d71e623/image.png" /></li>
</ul>
<pre><code class="language-python"># Rank(4)
prompt = template.format(
    instruction=&quot;Explain the process of photosynthesis in a way that a child could understand.&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/aff02c98-7342-41bc-83ea-9d0229bef418/image.png" /></p>
<pre><code class="language-python">
# Rank(8)
prompt = template.format(
    instruction=&quot;Explain the process of photosynthesis in a way that a child could understand.&quot;,
    response=&quot;&quot;,
)
print(gemma_lm.generate(prompt, max_length=256))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/7950ae42-5584-4570-a19f-84c90a9d3c3a/image.png" /></p>
<p>ë­í¬4ë³´ë‹¤ëŠ” ë­í¬8ë¡œ ë³€ê²½ì‹œ ê²°ê³¼ê°€ ë” ì¢‹ì€ê±° ê°™ë‹¤ <del>~</del></p>
<p><strong>ğŸ’¡ ë†’ì€ LoRA ìˆœìœ„</strong>ëŠ” ëª¨ë¸ì˜ ì„¸ë¶€ ì¡°ì •ê³¼ ì„±ëŠ¥ í–¥ìƒì— íš¨ê³¼ì ì´ë©°, ë³µì¡í•œ ì‘ì—…ê³¼ ë°ì´í„°ì…‹ì—ì„œ íŠ¹íˆ ìœ ìš©í•¨. í•˜ì§€ë§Œ ë¦¬ì†ŒìŠ¤ì™€ ì„±ëŠ¥ ê°„ì˜ ê· í˜•ì„ ê³ ë ¤í•´ì•¼ í•¨.</p>
<ul>
<li>LoRAë¥¼ í™œì„±í™”í•˜ë©´ í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ê°€ í¬ê²Œ ê°ì†Œí•¨</li>
<li>í•™ìŠµ ê°€ëŠ¥í•œ ë§¤ê°œë³€ìˆ˜ì˜ ìˆ˜ê°€ <strong>25ì–µ ê°œì—ì„œ 130ë§Œ ê°œ</strong>ë¡œ ì¤„ì–´ë“¤ìŒ</li>
</ul>
