---
title: "Text Classification using CNN"
date: Mon, 02 Dec 2024 01:26:23 GMT
categories: Velog
link: https://velog.io/@kim_taixi/Text-Classification-using-CNN
---

<h1 id="ë¶€ì œ-text-classification-using-cnn-acc-95">ë¶€ì œ: Text classification using CNN acc 95%</h1>
<blockquote>
<h2 id="íŒŒì¼-ì„¤ëª…">íŒŒì¼ ì„¤ëª…</h2>
</blockquote>
<ul>
<li>ì´ëª¨í‹°ì½˜ì„ í†µí•œ ê°ì •ë¶„ì„</li>
<li><strong>Emotions dataset for NLPì´ë¼ëŠ” ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•¨</strong></li>
</ul>
<blockquote>
<h2 id="ë¼ì´ë¸ŒëŸ¬ë¦¬-ë°-íŒ¨í‚¤ì§€-ì„¤ì¹˜">ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜</h2>
</blockquote>
<pre><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.layers import Dense, ReLU
from tensorflow.keras.layers import Embedding, BatchNormalization, Concatenate
from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dropout
from tensorflow.keras.models import Sequential, Model
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model</code></pre>
<blockquote>
<h2 id="ë°ì´í„°ë¡œë“œ">ë°ì´í„°ë¡œë“œ</h2>
</blockquote>
<pre><code class="language-python">df = pd.read_csv(&quot;/input/emotions-dataset-for-nlp/train.txt&quot;,
                 delimiter=';', header=None, names=['sentence','label'])

val_df = pd.read_csv(&quot;/input/emotions-dataset-for-nlp/val.txt&quot;,
                 delimiter=';', header=None, names=['sentence','label'])

ts_df = pd.read_csv(&quot;/input/emotions-dataset-for-nlp/test.txt&quot;,
                 delimiter=';', header=None, names=['sentence','label'])</code></pre>
<ul>
<li>í…ŒìŠ¤íŠ¸, í›ˆë ¨ ê²€ì¦ ë°ì´íŠ¸ë¥¼ ê°ê° inputí•´ì£¼ì—ˆìŒ.</li>
</ul>
<blockquote>
<h2 id="ë°ì´í„°-ì‚´í´ë³´ê¸°">ë°ì´í„° ì‚´í´ë³´ê¸°</h2>
</blockquote>
<pre><code class="language-python">df
df['label'].unique()
df.label.value_counts()</code></pre>
<ul>
<li><p>df
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/2db3b846-5b81-45f6-8ba7-a46dfa27fcab/image.png" /></p>
</li>
<li><p>df['label'].unique()
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/825c2468-402f-42d5-a036-38be87c9a7ee/image.png" /></p>
</li>
<li><p>df.label.value_counts()
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/1465ff62-19b5-49cf-b518-0c9f68b1f197/image.png" /></p>
</li>
<li><p>ë°ì´í„°ê°€ ê³ ë¥´ì§€ ëª»í•œê²ƒì´ ë³´ì„ìœ¼ë¡œ,ê°œì„  í•„ìš”</p>
</li>
</ul>
<blockquote>
<h2 id="ë°ì´í„°-ì‹œê°í™”">ë°ì´í„° ì‹œê°í™”</h2>
</blockquote>
<pre><code class="language-python">label_counts = df['label'].value_counts()
light_colors = sns.husl_palette(n_colors=len(label_counts))
sns.set(style=&quot;whitegrid&quot;)
plt.figure(figsize=(8, 8))
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=light_colors)
plt.title('Emotion Train Distribution')
plt.show()


label_counts = val_df['label'].value_counts()
light_colors = sns.husl_palette(n_colors=len(label_counts))
sns.set(style=&quot;whitegrid&quot;)
plt.figure(figsize=(8, 8))
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=light_colors)
plt.title('Emotion Valid Distribution')
plt.show()

label_counts = ts_df['label'].value_counts()
light_colors = sns.husl_palette(n_colors=len(label_counts))
sns.set(style=&quot;whitegrid&quot;)
plt.figure(figsize=(8, 8))
plt.pie(label_counts, labels=label_counts.index, autopct='%1.1f%%', startangle=140, colors=light_colors)
plt.title('Emotion Test Distribution')
plt.show()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/5fbb4578-6ac2-44a8-b270-c089256d2215/image.png" />
<strong>í›ˆë ¨ë°ì´í„°</strong></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/4bb980a0-a9fa-4be4-827e-7b7e5d1188ba/image.png" />
<strong>ê²€ì¦ë°ì´í„°</strong></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/2e155807-b8bb-4c17-9e2d-c0212a1c298a/image.png" />
<strong>í…ŒìŠ¤íŠ¸ ë°ì´í„°</strong></p>
<blockquote>
<h2 id="ë°ì´í„°-ì „ì²˜ë¦¬">ë°ì´í„° ì „ì²˜ë¦¬</h2>
</blockquote>
<pre><code class="language-python"># í›ˆë ¨ ë¶€ë¶„ 
df = df[~df['label'].str.contains('love')]
df = df[~df['label'].str.contains('surprise')]

joy = df[df['label'] == 'joy'].sample(n=2200, random_state=20)
sad = df[df['label'] == 'sadness'].sample(n=2200, random_state=20)
fear = df[df['label'] == 'fear'].sample(n=1937, random_state=20)
anger = df[df['label'] == 'anger'].sample(n=2159, random_state=20)

df_sampled = pd.concat([joy, sad, fear, anger])

df = df_sampled.sample(frac=1, random_state=20).reset_index(drop=True)

df.label.value_counts()
# ê²€ì¦ ë¶€ë¶„ 
val_df = val_df[~val_df['label'].str.contains('love')]
val_df = val_df[~val_df['label'].str.contains('surprise')]

joy = val_df[val_df['label'] == 'joy'].sample(n=250, random_state=20)
sad = val_df[val_df['label'] == 'sadness'].sample(n=250, random_state=20)
fear = val_df[val_df['label'] == 'fear'].sample(n=212, random_state=20)
anger = val_df[val_df['label'] == 'anger'].sample(n=275, random_state=20)

df_sampled = pd.concat([joy, sad, fear, anger])

val_df = df_sampled.sample(frac=1, random_state=20).reset_index(drop=True)
# í…ŒìŠ¤íŠ¸ ë¶€ë¶„ 
ts_df = ts_df[~ts_df['label'].str.contains('love')]
ts_df = ts_df[~ts_df['label'].str.contains('surprise')]

joy = ts_df[ts_df['label'] == 'joy'].sample(n=250, random_state=20)
sad = ts_df[ts_df['label'] == 'sadness'].sample(n=250, random_state=20)
fear = ts_df[ts_df['label'] == 'fear'].sample(n=224, random_state=20)
anger = ts_df[ts_df['label'] == 'anger'].sample(n=275, random_state=20)

df_sampled = pd.concat([joy, sad, fear, anger])

ts_df = df_sampled.sample(frac=1, random_state=20).reset_index(drop=True)</code></pre>
<ul>
<li>ë¶ˆê· í˜•í•œ ë°ì´í„° ì„¸íŠ¸ê°€ ìˆìœ¼ë¯€ë¡œ ì„œí”„ë¼ì´ì¦ˆ ë¼ë²¨ê³¼ ëŸ¬ë¸Œ ë¼ë²¨ì´ ê°€ì¥ ë‚®ê¸° ë•Œë¬¸ì— ë¼ë²¨ì„ ëª¨ë‘ ì œê±°</li>
</ul>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/12a35d5d-0e41-42b9-9ee9-fb1d0e99aae3/image.png" /></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/0031e9ee-46e7-42b9-9efc-2b6825d5624b/image.png" /></p>
<ul>
<li>ë¶ˆê· ë“±í•œ ë°ì´í„° ì œê±° ë° í‰ê· í™” ì‘ì—… ê²°ê³¼ </li>
</ul>
<blockquote>
<h2 id="ë°ì´í„°-ì „ì²˜ë¦¬2">ë°ì´í„° ì „ì²˜ë¦¬2</h2>
</blockquote>
<pre><code class="language-python"># Split data into X, y
tr_text = df['sentence']
tr_label = df['label']

val_text = val_df['sentence']
val_label = val_df['label']

ts_text = ts_df['sentence']
ts_label = ts_df['label']

# Encoding
encoder = LabelEncoder()
tr_label = encoder.fit_transform(tr_label)
val_label = encoder.transform(val_label)
ts_label = encoder.transform(ts_label)

# ì „ì²˜ë¦¬
tokenizer = Tokenizer(num_words=10000)
tokenizer.fit_on_texts(tr_text)

sequences = tokenizer.texts_to_sequences(tr_text)

tr_x = pad_sequences(sequences, maxlen=50)
tr_y = to_categorical(tr_label)

sequences = tokenizer.texts_to_sequences(val_text)
val_x = pad_sequences(sequences, maxlen=50)
val_y = to_categorical(val_label)

sequences = tokenizer.texts_to_sequences(ts_text)
ts_x = pad_sequences(sequences, maxlen=50)
ts_y = to_categorical(ts_label)</code></pre>
<blockquote>
<h2 id="ëª¨ë¸-êµ¬ì„±-ë°-í›ˆë ¨">ëª¨ë¸ êµ¬ì„± ë° í›ˆë ¨</h2>
</blockquote>
<pre><code class="language-python"># ëª¨ë¸ í›ˆë ¨ ë° êµ¬ì„±
max_words = 10000
max_len = 50
embedding_dim = 32

# Branch 1 Input
branch1_input = Input(shape=(max_len,))
branch1 = Embedding(max_words, embedding_dim, input_length=max_len)(branch1_input)
branch1 = Conv1D(64, 3, padding='same', activation='relu')(branch1)
branch1 = BatchNormalization()(branch1)
branch1 = ReLU()(branch1)
branch1 = Dropout(0.5)(branch1)
branch1 = GlobalMaxPooling1D()(branch1)

# Branch 2 Input
branch2_input = Input(shape=(max_len,))
branch2 = Embedding(max_words, embedding_dim, input_length=max_len)(branch2_input)
branch2 = Conv1D(64, 3, padding='same', activation='relu')(branch2)
branch2 = BatchNormalization()(branch2)
branch2 = ReLU()(branch2)
branch2 = Dropout(0.5)(branch2)
branch2 = GlobalMaxPooling1D()(branch2)

# Concatenate outputs
concatenated = Concatenate()([branch1, branch2])

# Define the rest of the model
hid_layer = Dense(128, activation='relu')(concatenated)
dropout = Dropout(0.3)(hid_layer)
output_layer = Dense(4, activation='softmax')(dropout)

# Create the model
model = Model(inputs=[branch1_input, branch2_input], outputs=output_layer)

model.compile(optimizer='adamax',
              loss='categorical_crossentropy',
              metrics=['accuracy', Precision(), Recall()])

model.summary()


batch_size = 256
epochs = 25
history = model.fit([tr_x, tr_x], tr_y, epochs=epochs, batch_size=batch_size,
                    validation_data=([val_x, val_x], val_y))</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/138b979c-73f0-4773-bd96-a4486b7f5ae7/image.png" /></p>
<ul>
<li>ëª¨ë¸ìƒì„±</li>
</ul>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/82df924f-58cb-46c8-b261-edfc61c49f96/image.png" /></p>
<blockquote>
<h2 id="ëª¨ë¸-êµ¬ì„±-ë°-í›ˆë ¨-1">ëª¨ë¸ êµ¬ì„± ë° í›ˆë ¨</h2>
</blockquote>
<pre><code class="language-python">tr_acc = history.history['accuracy']
tr_loss = history.history['loss']
tr_per = history.history['precision']
tr_recall = history.history['recall']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
val_per = history.history['val_precision']
val_recall = history.history['val_recall']

index_loss = np.argmin(val_loss)
val_lowest = val_loss[index_loss]
index_acc = np.argmax(val_acc)
acc_highest = val_acc[index_acc]
index_precision = np.argmax(val_per)
per_highest = val_per[index_precision]
index_recall = np.argmax(val_recall)
recall_highest = val_recall[index_recall]


Epochs = [i + 1 for i in range(len(tr_acc))]
loss_label = f'Best epoch = {str(index_loss + 1)}'
acc_label = f'Best epoch = {str(index_acc + 1)}'
per_label = f'Best epoch = {str(index_precision + 1)}'
recall_label = f'Best epoch = {str(index_recall + 1)}'


plt.figure(figsize=(20, 12))
plt.style.use('fivethirtyeight')

plt.subplot(2, 2, 1)
plt.plot(Epochs, tr_loss, 'r', label='Training loss')
plt.plot(Epochs, val_loss, 'g', label='Validation loss')
plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')
plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')
plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 3)
plt.plot(Epochs, tr_per, 'r', label='Precision')
plt.plot(Epochs, val_per, 'g', label='Validation Precision')
plt.scatter(index_precision + 1, per_highest, s=150, c='blue', label=per_label)
plt.title('Precision and Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 4)
plt.plot(Epochs, tr_recall, 'r', label='Recall')
plt.plot(Epochs, val_recall, 'g', label='Validation Recall')
plt.scatter(index_recall + 1, recall_highest, s=150, c='blue', label=recall_label)
plt.title('Recall and Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()
plt.grid(True)

plt.suptitle('Model Training Metrics Over Epochs', fontsize=16)
plt.show()tr_acc = history.history['accuracy']
tr_loss = history.history['loss']
tr_per = history.history['precision']
tr_recall = history.history['recall']
val_acc = history.history['val_accuracy']
val_loss = history.history['val_loss']
val_per = history.history['val_precision']
val_recall = history.history['val_recall']

index_loss = np.argmin(val_loss)
val_lowest = val_loss[index_loss]
index_acc = np.argmax(val_acc)
acc_highest = val_acc[index_acc]
index_precision = np.argmax(val_per)
per_highest = val_per[index_precision]
index_recall = np.argmax(val_recall)
recall_highest = val_recall[index_recall]


Epochs = [i + 1 for i in range(len(tr_acc))]
loss_label = f'Best epoch = {str(index_loss + 1)}'
acc_label = f'Best epoch = {str(index_acc + 1)}'
per_label = f'Best epoch = {str(index_precision + 1)}'
recall_label = f'Best epoch = {str(index_recall + 1)}'


plt.figure(figsize=(20, 12))
plt.style.use('fivethirtyeight')

plt.subplot(2, 2, 1)
plt.plot(Epochs, tr_loss, 'r', label='Training loss')
plt.plot(Epochs, val_loss, 'g', label='Validation loss')
plt.scatter(index_loss + 1, val_lowest, s=150, c='blue', label=loss_label)
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(Epochs, tr_acc, 'r', label='Training Accuracy')
plt.plot(Epochs, val_acc, 'g', label='Validation Accuracy')
plt.scatter(index_acc + 1, acc_highest, s=150, c='blue', label=acc_label)
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 3)
plt.plot(Epochs, tr_per, 'r', label='Precision')
plt.plot(Epochs, val_per, 'g', label='Validation Precision')
plt.scatter(index_precision + 1, per_highest, s=150, c='blue', label=per_label)
plt.title('Precision and Validation Precision')
plt.xlabel('Epochs')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 4)
plt.plot(Epochs, tr_recall, 'r', label='Recall')
plt.plot(Epochs, val_recall, 'g', label='Validation Recall')
plt.scatter(index_recall + 1, recall_highest, s=150, c='blue', label=recall_label)
plt.title('Recall and Validation Recall')
plt.xlabel('Epochs')
plt.ylabel('Recall')
plt.legend()
plt.grid(True)

plt.suptitle('Model Training Metrics Over Epochs', fontsize=16)
plt.show()</code></pre>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/3c00614f-42d4-4fe3-905f-bb9a03bb843e/image.png" /></p>
<p><strong>Training and Validation Loss</strong></p>
<ul>
<li>Training Lossì™€ Validation Lossë¥¼ ì—í¬í¬(epoch)ë³„ë¡œ ë¹„êµí•œ ê·¸ë˜í”„</li>
<li>ì´ˆê¸°ì—ëŠ” Training Lossì™€ Validation Loss ëª¨ë‘ ë†’ì§€ë§Œ, ì ì°¨ì ìœ¼ë¡œ ê°ì†Œí•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ë‚®ì€ ì†ì‹¤ì„ ë³´ì„.</li>
<li>ì—í¬í¬ê°€ ì§„í–‰ë ìˆ˜ë¡ ë‘ ì†ì‹¤ì˜ ê°’ì´ ë¹„ìŠ·í•´ì§€ë©°, 25ë²ˆì§¸ ì—í¬í¬ì—ì„œ ìµœì ì˜ ì†ì‹¤(Best epoch = 25)ì„ ê¸°ë¡.</li>
<li>Validation Lossê°€ Training Lossë³´ë‹¤ ì¡°ê¸ˆ ë†’ì€ë°, ì´ëŠ” ê³¼ì í•©(overfitting)ì´ ì¼ë¶€ ë°œìƒí–ˆìŒì„ ì‹œì‚¬í•  ìˆ˜ ìˆìŒ.</li>
</ul>
<p><strong>Training and Validation Accuracy</strong></p>
<ul>
<li>Training Accuracyì™€ Validation Accuracyë¥¼ ì—í¬í¬ë³„ë¡œ ë¹„êµí•œ ê·¸ë˜í”„
ì—í¬í¬ê°€ ì¦ê°€í•¨ì— ë”°ë¼ ë‘ ì •í™•ë„ê°€ ìƒìŠ¹í•˜ê³ , ê±°ì˜ ì¼ì¹˜í•˜ëŠ” ëª¨ìŠµì„ ë³´ì„.</li>
<li>25ë²ˆì§¸ ì—í¬í¬ì—ì„œ ìµœê³ ì˜ ì •í™•ë„ë¥¼ ê¸°ë¡í•˜ê³  ìˆìŠµë‹ˆë‹¤ (Best epoch = 25).</li>
<li>ì „ë°˜ì ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ë¥¼ ì˜ í•™ìŠµí–ˆìœ¼ë©° ê²€ì¦ ë°ì´í„°ì— ëŒ€í•´ì„œë„ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ</li>
</ul>
<p><strong>Precision and Validation Precision</strong></p>
<ul>
<li>ì •ë°€ë„(Precision)ì™€ ê²€ì¦ ì •ë°€ë„(Validation Precision)ë¥¼ ë¹„êµí•œ ê·¸ë˜í”„</li>
<li>ê²€ì¦ ì •ë°€ë„ê°€ 12ë²ˆì§¸ ì—í¬í¬ì—ì„œ ë§¤ìš° ê¸‰ê²©íˆ ìƒìŠ¹í•˜ê³  ìˆìœ¼ë©° (Best epoch = 12), ì´í›„ ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ê³  ìˆìŒ</li>
<li>ì´ëŠ” ëª¨ë¸ì´ 12ë²ˆì§¸ ì—í¬í¬ ì´í›„ íŠ¹ì • íŒ¨í„´ì„ ì˜ í•™ìŠµí–ˆìŒì„ ì˜ë¯¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹¤ë§Œ <strong>ê²€ì¦ ì •ë°€ë„ì˜ ê¸‰ê²©í•œ ìƒìŠ¹ ì´í›„ ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ëŠ” ì ì´ ë‹¤ì†Œ ë¹„ì •ìƒì ìœ¼ë¡œ ë³´ì¼ ìˆ˜ ìˆìœ¼ë©°, ê³¼ì í•© ë¬¸ì œ</strong>ë¥¼ ì˜ì‹¬í•´ ë³¼ í•„ìš”ê°€ ìˆìŒ</li>
</ul>
<p><strong>Recall and Validation Recall</strong></p>
<ul>
<li>ì¬í˜„ìœ¨(Recall)ê³¼ ê²€ì¦ ì¬í˜„ìœ¨(Validation Recall)ì„ ë¹„êµí•œ ê·¸ë˜í”„</li>
<li>ì¬í˜„ìœ¨ì€ ì ì§„ì ìœ¼ë¡œ ì¦ê°€í•˜ë©° 24ë²ˆì§¸ ì—í¬í¬ì—ì„œ ìµœê³ (Best epoch = 24)ë¥¼ ê¸°ë¡</li>
<li>Validation Recallì€ ì´ˆê¸°ì— ë§¤ìš° ë‚®ì§€ë§Œ ì—í¬í¬ê°€ ì§„í–‰ë¨ì— ë”°ë¼ ê¸‰ê²©íˆ ì¦ê°€</li>
<li>ì¬í˜„ìœ¨ì´ ì•ˆì •ì ìœ¼ë¡œ ì¦ê°€í•œ ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ê²€ì¦ ì¬í˜„ìœ¨ì´ ì´ˆê¸°ì— ë§¤ìš° ë‚®ê³  ë‚˜ì¤‘ì— ê¸‰ê²©íˆ ì¦ê°€í•˜ëŠ” ì–‘ìƒì„ ë³´ì´ëŠ” ê²ƒì€ <strong>ë°ì´í„°ì˜ ë¶ˆê· í˜•ì´ë‚˜ ëª¨ë¸ì˜ í•™ìŠµ ë¬¸ì œ</strong>ë¥¼ ì‹œì‚¬í•  ìˆ˜ ìˆìŒ</li>
</ul>
<h3 id="ì¢…í•©">ì¢…í•©</h3>
<ul>
<li>ëª¨ë¸ì€ ì „ë°˜ì ìœ¼ë¡œ í•™ìŠµ ì†ì‹¤ê³¼ ì •í™•ë„ê°€ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œí•˜ê³  ì¦ê°€í•˜ê³  ìˆì§€ë§Œ, ì¼ë¶€ ê³¼ì í•©ì˜ ì§•í›„</li>
<li>ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ ì¸¡ë©´ì—ì„œ ê²€ì¦ ì§€í‘œì˜ ë³€í™”ê°€ ê¸‰ê²©í•˜ê³ , ì¼ì •í•˜ê²Œ ìœ ì§€ë˜ëŠ” íŒ¨í„´ì´ ë‚˜íƒ€ë‚˜ë©° ì´ëŠ” ê³¼ì í•©ì´ë‚˜ ë°ì´í„° ë¬¸ì œë¥¼ ì‹œì‚¬í•  ìˆ˜ ìˆìŒ</li>
<li>ìµœì ì˜ ì—í¬í¬(Best epoch)ëŠ” ê° ì§€í‘œë§ˆë‹¤ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ë¯€ë¡œ, ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ Validation Lossì™€ Validation Accuracyì˜ ìµœì  ì—í¬í¬(25)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒìœ¼ë¡œ ë³´ì„</li>
</ul>
<h4 id="í•´ê²°ë°©ì•ˆ">í•´ê²°ë°©ì•ˆ</h4>
<p>ì´ ê·¸ë˜í”„ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê³¼ì í•©ì„ ì¤„ì´ê¸° ìœ„í•´ ë“œë¡­ì•„ì›ƒ(dropout)ì´ë‚˜ ì¡°ê¸° ì¢…ë£Œ(early stopping) ë“±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŒ</p>
<blockquote>
<h3 id="ì˜ˆì¸¡-ê²°ê³¼ë¥¼-ì‹œê°í™”">ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì‹œê°í™”</h3>
</blockquote>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/61184bd5-020e-4ae5-a669-510ca05162df/image.png" />
<img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/2cbd0050-8bcc-4858-b39f-e3cb4b96c1d9/image.png" /></p>
<p><img alt="" src="https://velog.velcdn.com/images/kim_taixi/post/460c8798-3bda-48c1-bae3-16dc0aa42cb3/image.png" /></p>
<h4 id="ğŸ’¡ë¬¸ì¥ì„-ì‹¤í—˜í•œ-ê²°ê³¼-ì˜ë‚˜ì˜¤ê³ -ë‹¤ì†Œ-ê³¼ê²©í•œ-ë¬¸ì¥ì„-ì‹¤í—˜í–ˆì§€ë§Œ-ì˜í–¥ì„-í¬ê²Œ-ì£¼ì§€ëŠ”-ì•ŠìŒ">ğŸ’¡ë¬¸ì¥ì„ ì‹¤í—˜í•œ ê²°ê³¼ ì˜ë‚˜ì˜¤ê³  ë‹¤ì†Œ ê³¼ê²©í•œ ë¬¸ì¥ì„ ì‹¤í—˜í–ˆì§€ë§Œ ì˜í–¥ì„ í¬ê²Œ ì£¼ì§€ëŠ” ì•ŠìŒ</h4>
<h3 id="ê²°ë¡ -ì•½ê°„ì˜-ëª¨ë¸-ìˆ˜ì •-ë°-ì—í­-ìˆ˜ì •ì´-í•„ìš”í•˜ë©´-ë”-ë‚˜ì€-ê²°ê³¼ê°€-ë‚˜ì˜¬-ê²ƒìœ¼ë¡œ-ì˜ˆìƒí•œë‹¤">ê²°ë¡ : ì•½ê°„ì˜ ëª¨ë¸ ìˆ˜ì • ë° ì—í­ ìˆ˜ì •ì´ í•„ìš”í•˜ë©´ ë” ë‚˜ì€ ê²°ê³¼ê°€ ë‚˜ì˜¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒí•œë‹¤.</h3>
